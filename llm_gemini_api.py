# -*- coding: utf-8 -*-
"""LLM-Gemini_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V51vr5jBS54udZBNH0fwmuna7IawESLP
"""

pip install google-generativeai pandas

pip tqdm

pip install --upgrade google-generativeai

import requests

url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
api_key = "Enter Your Tokens"  # Replace with your own

headers = {
    "Content-Type": "application/json",
    "X-goog-api-key": api_key
}

payload = {
    "contents": [
        {
            "parts": [
                {
                    "text": "Explain how AI works in a few words"
                }
            ]
        }
    ]
}

response = requests.post(url, headers=headers, json=payload)
print(response.json())

import pandas as pd
import google.generativeai as genai

# 🔐 Configure Gemini API
genai.configure(api_key="Enter your tokens")  # Use your valid API key

# 🧠 Load your datasetAIzaSyCRUQMSdr7lth7mYXx83Ndhlx7p30r3KRE
df = pd.read_csv("/content/Total Linkein data(in).csv")

# 📊 Filter valid rows to avoid NaNs
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✅ Select a few examples for few-shot prompting
examples = df.sample(min(3, len(df)), random_state=42)

# 🧱 Build the few-shot prompt
few_shot_prompt = ""
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Profile:
  Name: {row['Name']}
  Title: {row['Title']}
  Location: {row['Location']}
  Post Type: {row['Type']}
  Post Content: {row['Content']}

👉 Suggested LinkedIn Caption:
  🚀 Meet {row['Name']}, a {row['Title']} from {row['Location']}!
  💼 {str(row['Content'])[:100]}...
  🔗 #Career #LinkedIn #Inspiration
---
"""

# 🎯 Pick a new row for caption generation
target_row = df.sample(1, random_state=1).iloc[0]

# 🧠 Create the final prompt
final_prompt = few_shot_prompt + f"""
Profile:
  Name: {target_row['Name']}
  Title: {target_row['Title']}
  Location: {target_row['Location']}
  Post Type: {target_row['Type']}
  Post Content: {target_row['Content']}

👉 Suggested LinkedIn Caption:
"""

# ✅ Call Gemini API using supported model
model = genai.GenerativeModel("models/gemini-2.0-flash")  # Correct model name
response = model.generate_content(final_prompt)

# 📢 Output Result
print("\n🔹 Generated LinkedIn Caption:\n")
print(response.text)

import pandas as pd
import google.generativeai as genai

# 🔐 Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual API key

# 📂 Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")

# 🧹 Clean the data: drop rows missing necessary info
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# 📌 Sample few examples to demonstrate style (few-shot prompting)
examples = df.sample(min(3, len(df)), random_state=42)

# 🧱 Build few-shot style prompt
few_shot_prompt = ""
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Input: {row['Content']}

Output:
🏆 Meet {row['Name']}, a {row['Title']} from {row['Location']}!
💼 {str(row['Content'])[:120]}...
✨ #Career #Motivation #LinkedInJourney
---
"""

# 🔤 Custom new input (your test case)
new_input = "50 days streak in LeetCode"

# 🧠 Build final prompt
final_prompt = few_shot_prompt + f"""
Input: {new_input}

Output:
"""

# 🤖 Use Gemini API with correct model name
model = genai.GenerativeModel("models/gemini-2.0-flash")  # ✅ Use valid Gemini model

# 🧠 Generate output
response = model.generate_content(final_prompt)

# 📢 Show the result
print("\n🔹 Generated LinkedIn Caption:\n")
print(response.text)

import pandas as pd
import google.generativeai as genai

# 🔐 Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your API key

# 📂 Load your LinkedIn dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")

# 🧹 Clean the dataset
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✂️ Pick a few rows for few-shot prompting
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Build few-shot prompt: input → full LinkedIn caption output
few_shot_prompt = "You are a professional LinkedIn caption generator. For each input, write an engaging caption with emojis and hashtags.\n\n"
for _, row in examples.iterrows():
    few_shot_prompt += f"""\
Input: {row['Content']}

Output:
🏆 Meet {row['Name']}, a {row['Title']} from {row['Location']}!
💼 {str(row['Content'])[:120]}...
✨ #Career #Motivation #LinkedInJourney
---
"""

# 🆕 User custom input
new_input = "50 days streak in LeetCode"

# 📥 Add the new input to the prompt
final_prompt = few_shot_prompt + f"""\
Input: {new_input}

Output:
"""

# 🧠 Use Gemini API with correct model name
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# 📢 Show the generated caption
print("\n🔹 Generated LinkedIn Caption:\n")
print(response.text.strip())

import pandas as pd
import google.generativeai as genai

# 🔐 Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual key

# 📂 Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✂️ Take a few examples to guide the model (few-shot)
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Instructional Prompt for High-Quality Caption
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2–3 line body
- 3 bullet points (check emoji ✅)
- Concluding motivation
- 🚀 ending
- 10+ descriptive hashtags starting with `hashtag#`

Keep it personal, enthusiastic, and detailed.

"""

# 🔁 Add real examples from your dataset (simplified for prompt style)
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
🏆 {row['Type']} at {row['Location']}!
What a journey it's been – {row['Name']} took one bold step forward.
✅ {row['Title']}
✅ Shared insights
✅ Inspired community
The impact continues to grow. Stay tuned! 🚀
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# 🎯 Your input example
new_input = "50 days streak in LeetCode"

# 🔧 Final Prompt
final_prompt = few_shot_prompt + f"""Input: {new_input}

Output:
"""

# ⚙️ Use the correct Gemini model
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# 📢 Display result
print("\n🔹 Generated LinkedIn Caption:\n")
print(response.text.strip())

import pandas as pd
import google.generativeai as genai

# 🔐 Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual key

# 📂 Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✂️ Take a few examples to guide the model (few-shot)
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Instructional Prompt for High-Quality Caption
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2–3 line body
- 3 bullet points (check emoji ✅)
- Concluding motivation
- 🚀 ending
- 10+ descriptive hashtags starting with `hashtag#`

Keep it personal, enthusiastic, and detailed.

"""

# 🔁 Add real examples from your dataset (simplified for prompt style)
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
🏆 {row['Type']} at {row['Location']}!
What a journey it's been – {row['Name']} took one bold step forward.
✅ {row['Title']}
✅ Shared insights
✅ Inspired community
The impact continues to grow. Stay tuned! 🚀
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# 🎯 Your input example
new_input = "joind in tcs"

# 🔧 Final Prompt
final_prompt = few_shot_prompt + f"""Input: {new_input}

Output:
"""

# ⚙️ Use the correct Gemini model
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# 📢 Display result
print("\n🔹 Generated LinkedIn Caption:\n")
print(response.text.strip())



"""
🔹 Generated LinkedIn Caption:

🏆 Exciting News!
What a journey it's been – TCS took one bold step forward.
✅ Expanded global reach
✅ Shared insights
✅ Inspired community
The impact continues to grow. Stay tuned! 🚀
hashtag#LinkedIn hashtag#TCS hashtag#Innovation hashtag#Technology hashtag#Growth hashtag#Community hashtag#Inspiration hashtag#Career hashtag#Success hashtag#DigitalTransformation




🔹 Generated LinkedIn Caption:

🏆 50 Days of LeetCode! 🚀

Crushing coding challenges, one day at a time! Consistency is key, and I'm thrilled to celebrate this milestone. Bring on the next 50! 💪

✅ Sharpened my problem-solving skills
✅ Enhanced my understanding of data structures and algorithms
✅ Built a stronger foundation for technical interviews

The journey of a thousand lines of code begins with a single commit. Keep coding, keep learning, and never give up! 🚀

hashtag#LeetCode hashtag#CodingChallenge hashtag#100DaysOfCode hashtag#DataStructures hashtag#Algorithms hashtag#SoftwareEngineering hashtag#ProblemSolving hashtag#CodingLife hashtag#Motivation hashtag#CareerGrowth hashtag#Tech"""

pip install transformers accelerate torch pandas

!pip install transformers datasets accelerate bitsandbytes
!pip install peft trl

import pandas as pd
from datasets import Dataset

# Load your LinkedIn data
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# Create prompts
def format_prompt(row):
    return {
        "prompt": f"""You are a professional LinkedIn caption writer.
Input: {row['Content']}

Write a human-like LinkedIn post:
- Title with 1 emoji
- Motivational short paragraph (avoid too many emojis)
- 3 bullet points (with ✅ emoji)
- Final sentence of motivation
- End with '🚀'
- Include 10+ detailed hashtags (start with hashtag#)""",
        "response": f"{row['Type']} update from {row['Name']} at {row['Location']}. Title: {row['Title']}."
    }

formatted = [format_prompt(row) for _, row in df.iterrows()]
dataset = Dataset.from_list(formatted)

'''from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

# Prepare dataset for training
def tokenize(example):
    return tokenizer(f"{example['prompt']}\n\nOutput:\n{example['response']}", truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize)

# LoRA config
lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"], lora_dropout=0.05, bias="none", task_type="CAUSAL_LM")

# Training args
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    num_train_epochs=2,
    learning_rate=5e-5,
    fp16=True,
    logging_steps=10,
    output_dir="./mistral-linkedin-finetuned",
    save_total_limit=2,
    save_strategy="epoch"
)

# Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=tokenized_dataset,
    peft_config=lora_config,
    args=training_args
)

# Train
trainer.train()'''

pip install transformers torch pandas

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# 📂 Load Dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# 🔁 Take few examples
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Few-shot Prompt Template
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a short achievement (Input), write a personal and inspiring LinkedIn caption with:
- Title with emoji
- 2–3 motivational lines
- 3 bullet points (check emoji ✅)
- Final line motivation
- Ends with 🚀
- 10+ hashtags starting with hashtag#

Keep the tone human, less robotic, and without too many emojis.

"""

for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}
Output:
🏆 {row['Type']} at {row['Location']}!
What a journey it's been – {row['Name']} took one bold step forward.
✅ {row['Title']}
✅ Shared insights
✅ Inspired community
The impact continues to grow. Stay tuned! 🚀
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# 🎯 New Input
new_input = "joined TCS as software engineer"
prompt = few_shot_prompt + f"Input: {new_input}\nOutput:\n"

# 🔧 Load Model + Tokenizer (free and open-source)
model_name = "google/flan-t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# ✨ Generate Caption
inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
outputs = model.generate(**inputs, max_new_tokens=250)
generated_caption = tokenizer.decode(outputs[0], skip_special_tokens=True)

# 📢 Result
print("\n🔹 Generated LinkedIn Caption:\n")
print(generated_caption.strip())

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ✅ Load model and tokenizer (FREE)
model_name = "google/flan-t5-base"  # You can try t5-base or flan-t5-xl (larger, better results)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 📂 Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✂️ Sample 3 examples from the dataset
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Instructional Prompt
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2–3 line body
- 3 bullet points (check emoji ✅)
- Concluding motivation
- 🚀 ending
- 10+ descriptive hashtags starting with hashtag#

Keep it personal, enthusiastic, and detailed.

"""

# 🔁 Add 3 few-shot examples
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
🏆 {row['Type']} at {row['Location']}!
What a journey it's been – {row['Name']} took one bold step forward.
✅ {row['Title']}
✅ Shared insights
✅ Inspired community
The impact continues to grow. Stay tuned! 🚀
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# 🔤 New input
new_input = "joined in TCS"

# 🎯 Final Prompt
final_prompt = few_shot_prompt + f"Input: {new_input}\n\nOutput:\n"

# 🧠 Tokenize and generate
inputs = tokenizer(final_prompt, return_tensors="pt", max_length=1024, truncation=True)
output_ids = model.generate(**inputs, max_new_tokens=250, temperature=0.7)
output = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# 📢 Show output
print("🔹 Generated LinkedIn Caption:\n")
print(output)

pip install transformers accelerate pandas

'''import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# 📂 Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ✂️ Take few examples
examples = df.sample(min(3, len(df)), random_state=42)

# 🧠 Prompt construction
few_shot_prompt = """You are a professional LinkedIn caption generator.

Generate a human-like paragraph-style caption for a LinkedIn post.

Requirements:
- Start with a personal and relatable hook (no emojis)
- Keep tone inspirational, not overly formal
- Avoid emoji overload (max 1-2 if any)
- Make it human, story-like, concise (4–6 lines)
- End with relevant hashtags (starting with 'hashtag#', around 10)

Examples:
"""

# 🔁 Add real examples from dataset
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Input: {row['Content']}

Output:
I recently achieved something meaningful in my journey: {row['Type']} at {row['Location']}.
It reminded me of how growth often comes from stepping outside our comfort zones.
I’m grateful for the lessons learned and the amazing people I connected with.
Let's keep pushing boundaries and sharing our progress.
hashtag#Career hashtag#Growth hashtag#Inspiration hashtag#LearningJourney
---
"""

# 🔢 New user input
new_input = "LeetCode 50 days streak"

# 📦 Final prompt
final_prompt = few_shot_prompt + f"\nInput: {new_input}\n\nOutput:\n"

# 🧠 Load tokenizer and model from Hugging Face (free model)
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# 🛠 Inference pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# ⚙️ Generate output
output = generator(final_prompt, max_new_tokens=300, do_sample=True, temperature=0.7)[0]['generated_text']

# 🧼 Clean output (print only what comes after "Output:")
print("\n🔹 Generated LinkedIn Caption:\n")
print(output.split("Output:")[-1].strip())'''

import requests

url = "https://openrouter.ai/api/v1/chat/completions"
headers = {
    "Authorization": "Your model or api",
    "Content-Type": "application/json",
    "HTTP-Referer": "https://your-site.com",  # Optional
    "X-Title": "My Test App"  # Optional
}
data = {
    "model": "meta-llama/llama-3-70b-instruct",
    "messages": [
        {"role": "user", "content": "What is the meaning of life?"}
    ]
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["choices"][0]["message"]["content"])

pip install openai

pip install transformers datasets scikit-learn torch

!pip install --upgrade transformers

pip install -U transformers

from torch.optim import AdamW

!pip install -U transformers

import transformers
print(transformers.__version__)

