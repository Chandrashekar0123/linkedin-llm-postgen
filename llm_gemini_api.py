# -*- coding: utf-8 -*-
"""LLM-Gemini_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V51vr5jBS54udZBNH0fwmuna7IawESLP
"""

pip install google-generativeai pandas

pip tqdm

pip install --upgrade google-generativeai

import requests

url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
api_key = "Enter Your Tokens"  # Replace with your own

headers = {
    "Content-Type": "application/json",
    "X-goog-api-key": api_key
}

payload = {
    "contents": [
        {
            "parts": [
                {
                    "text": "Explain how AI works in a few words"
                }
            ]
        }
    ]
}

response = requests.post(url, headers=headers, json=payload)
print(response.json())

import pandas as pd
import google.generativeai as genai

# ğŸ” Configure Gemini API
genai.configure(api_key="Enter your tokens")  # Use your valid API key

# ğŸ§  Load your datasetAIzaSyCRUQMSdr7lth7mYXx83Ndhlx7p30r3KRE
df = pd.read_csv("/content/Total Linkein data(in).csv")

# ğŸ“Š Filter valid rows to avoid NaNs
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ… Select a few examples for few-shot prompting
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§± Build the few-shot prompt
few_shot_prompt = ""
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Profile:
  Name: {row['Name']}
  Title: {row['Title']}
  Location: {row['Location']}
  Post Type: {row['Type']}
  Post Content: {row['Content']}

ğŸ‘‰ Suggested LinkedIn Caption:
  ğŸš€ Meet {row['Name']}, a {row['Title']} from {row['Location']}!
  ğŸ’¼ {str(row['Content'])[:100]}...
  ğŸ”— #Career #LinkedIn #Inspiration
---
"""

# ğŸ¯ Pick a new row for caption generation
target_row = df.sample(1, random_state=1).iloc[0]

# ğŸ§  Create the final prompt
final_prompt = few_shot_prompt + f"""
Profile:
  Name: {target_row['Name']}
  Title: {target_row['Title']}
  Location: {target_row['Location']}
  Post Type: {target_row['Type']}
  Post Content: {target_row['Content']}

ğŸ‘‰ Suggested LinkedIn Caption:
"""

# âœ… Call Gemini API using supported model
model = genai.GenerativeModel("models/gemini-2.0-flash")  # Correct model name
response = model.generate_content(final_prompt)

# ğŸ“¢ Output Result
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(response.text)

import pandas as pd
import google.generativeai as genai

# ğŸ” Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual API key

# ğŸ“‚ Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")

# ğŸ§¹ Clean the data: drop rows missing necessary info
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ğŸ“Œ Sample few examples to demonstrate style (few-shot prompting)
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§± Build few-shot style prompt
few_shot_prompt = ""
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Input: {row['Content']}

Output:
ğŸ† Meet {row['Name']}, a {row['Title']} from {row['Location']}!
ğŸ’¼ {str(row['Content'])[:120]}...
âœ¨ #Career #Motivation #LinkedInJourney
---
"""

# ğŸ”¤ Custom new input (your test case)
new_input = "50 days streak in LeetCode"

# ğŸ§  Build final prompt
final_prompt = few_shot_prompt + f"""
Input: {new_input}

Output:
"""

# ğŸ¤– Use Gemini API with correct model name
model = genai.GenerativeModel("models/gemini-2.0-flash")  # âœ… Use valid Gemini model

# ğŸ§  Generate output
response = model.generate_content(final_prompt)

# ğŸ“¢ Show the result
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(response.text)

import pandas as pd
import google.generativeai as genai

# ğŸ” Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your API key

# ğŸ“‚ Load your LinkedIn dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")

# ğŸ§¹ Clean the dataset
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ‚ï¸ Pick a few rows for few-shot prompting
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Build few-shot prompt: input â†’ full LinkedIn caption output
few_shot_prompt = "You are a professional LinkedIn caption generator. For each input, write an engaging caption with emojis and hashtags.\n\n"
for _, row in examples.iterrows():
    few_shot_prompt += f"""\
Input: {row['Content']}

Output:
ğŸ† Meet {row['Name']}, a {row['Title']} from {row['Location']}!
ğŸ’¼ {str(row['Content'])[:120]}...
âœ¨ #Career #Motivation #LinkedInJourney
---
"""

# ğŸ†• User custom input
new_input = "50 days streak in LeetCode"

# ğŸ“¥ Add the new input to the prompt
final_prompt = few_shot_prompt + f"""\
Input: {new_input}

Output:
"""

# ğŸ§  Use Gemini API with correct model name
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# ğŸ“¢ Show the generated caption
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(response.text.strip())

import pandas as pd
import google.generativeai as genai

# ğŸ” Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual key

# ğŸ“‚ Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ‚ï¸ Take a few examples to guide the model (few-shot)
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Instructional Prompt for High-Quality Caption
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2â€“3 line body
- 3 bullet points (check emoji âœ…)
- Concluding motivation
- ğŸš€ ending
- 10+ descriptive hashtags starting with `hashtag#`

Keep it personal, enthusiastic, and detailed.

"""

# ğŸ” Add real examples from your dataset (simplified for prompt style)
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
ğŸ† {row['Type']} at {row['Location']}!
What a journey it's been â€“ {row['Name']} took one bold step forward.
âœ… {row['Title']}
âœ… Shared insights
âœ… Inspired community
The impact continues to grow. Stay tuned! ğŸš€
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# ğŸ¯ Your input example
new_input = "50 days streak in LeetCode"

# ğŸ”§ Final Prompt
final_prompt = few_shot_prompt + f"""Input: {new_input}

Output:
"""

# âš™ï¸ Use the correct Gemini model
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# ğŸ“¢ Display result
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(response.text.strip())

import pandas as pd
import google.generativeai as genai

# ğŸ” Configure Gemini API
genai.configure(api_key="Enter Your Tokens")  # Replace with your actual key

# ğŸ“‚ Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ‚ï¸ Take a few examples to guide the model (few-shot)
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Instructional Prompt for High-Quality Caption
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2â€“3 line body
- 3 bullet points (check emoji âœ…)
- Concluding motivation
- ğŸš€ ending
- 10+ descriptive hashtags starting with `hashtag#`

Keep it personal, enthusiastic, and detailed.

"""

# ğŸ” Add real examples from your dataset (simplified for prompt style)
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
ğŸ† {row['Type']} at {row['Location']}!
What a journey it's been â€“ {row['Name']} took one bold step forward.
âœ… {row['Title']}
âœ… Shared insights
âœ… Inspired community
The impact continues to grow. Stay tuned! ğŸš€
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# ğŸ¯ Your input example
new_input = "joind in tcs"

# ğŸ”§ Final Prompt
final_prompt = few_shot_prompt + f"""Input: {new_input}

Output:
"""

# âš™ï¸ Use the correct Gemini model
model = genai.GenerativeModel("models/gemini-2.0-flash")
response = model.generate_content(final_prompt)

# ğŸ“¢ Display result
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(response.text.strip())



"""
ğŸ”¹ Generated LinkedIn Caption:

ğŸ† Exciting News!
What a journey it's been â€“ TCS took one bold step forward.
âœ… Expanded global reach
âœ… Shared insights
âœ… Inspired community
The impact continues to grow. Stay tuned! ğŸš€
hashtag#LinkedIn hashtag#TCS hashtag#Innovation hashtag#Technology hashtag#Growth hashtag#Community hashtag#Inspiration hashtag#Career hashtag#Success hashtag#DigitalTransformation




ğŸ”¹ Generated LinkedIn Caption:

ğŸ† 50 Days of LeetCode! ğŸš€

Crushing coding challenges, one day at a time! Consistency is key, and I'm thrilled to celebrate this milestone. Bring on the next 50! ğŸ’ª

âœ… Sharpened my problem-solving skills
âœ… Enhanced my understanding of data structures and algorithms
âœ… Built a stronger foundation for technical interviews

The journey of a thousand lines of code begins with a single commit. Keep coding, keep learning, and never give up! ğŸš€

hashtag#LeetCode hashtag#CodingChallenge hashtag#100DaysOfCode hashtag#DataStructures hashtag#Algorithms hashtag#SoftwareEngineering hashtag#ProblemSolving hashtag#CodingLife hashtag#Motivation hashtag#CareerGrowth hashtag#Tech"""

pip install transformers accelerate torch pandas

!pip install transformers datasets accelerate bitsandbytes
!pip install peft trl

import pandas as pd
from datasets import Dataset

# Load your LinkedIn data
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# Create prompts
def format_prompt(row):
    return {
        "prompt": f"""You are a professional LinkedIn caption writer.
Input: {row['Content']}

Write a human-like LinkedIn post:
- Title with 1 emoji
- Motivational short paragraph (avoid too many emojis)
- 3 bullet points (with âœ… emoji)
- Final sentence of motivation
- End with 'ğŸš€'
- Include 10+ detailed hashtags (start with hashtag#)""",
        "response": f"{row['Type']} update from {row['Name']} at {row['Location']}. Title: {row['Title']}."
    }

formatted = [format_prompt(row) for _, row in df.iterrows()]
dataset = Dataset.from_list(formatted)

'''from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

# Prepare dataset for training
def tokenize(example):
    return tokenizer(f"{example['prompt']}\n\nOutput:\n{example['response']}", truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize)

# LoRA config
lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=["q_proj", "v_proj"], lora_dropout=0.05, bias="none", task_type="CAUSAL_LM")

# Training args
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    num_train_epochs=2,
    learning_rate=5e-5,
    fp16=True,
    logging_steps=10,
    output_dir="./mistral-linkedin-finetuned",
    save_total_limit=2,
    save_strategy="epoch"
)

# Trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=tokenized_dataset,
    peft_config=lora_config,
    args=training_args
)

# Train
trainer.train()'''

pip install transformers torch pandas

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

# ğŸ“‚ Load Dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# ğŸ” Take few examples
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Few-shot Prompt Template
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a short achievement (Input), write a personal and inspiring LinkedIn caption with:
- Title with emoji
- 2â€“3 motivational lines
- 3 bullet points (check emoji âœ…)
- Final line motivation
- Ends with ğŸš€
- 10+ hashtags starting with hashtag#

Keep the tone human, less robotic, and without too many emojis.

"""

for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}
Output:
ğŸ† {row['Type']} at {row['Location']}!
What a journey it's been â€“ {row['Name']} took one bold step forward.
âœ… {row['Title']}
âœ… Shared insights
âœ… Inspired community
The impact continues to grow. Stay tuned! ğŸš€
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# ğŸ¯ New Input
new_input = "joined TCS as software engineer"
prompt = few_shot_prompt + f"Input: {new_input}\nOutput:\n"

# ğŸ”§ Load Model + Tokenizer (free and open-source)
model_name = "google/flan-t5-base"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# âœ¨ Generate Caption
inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True)
outputs = model.generate(**inputs, max_new_tokens=250)
generated_caption = tokenizer.decode(outputs[0], skip_special_tokens=True)

# ğŸ“¢ Result
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(generated_caption.strip())

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# âœ… Load model and tokenizer (FREE)
model_name = "google/flan-t5-base"  # You can try t5-base or flan-t5-xl (larger, better results)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# ğŸ“‚ Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ‚ï¸ Sample 3 examples from the dataset
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Instructional Prompt
few_shot_prompt = """You are a professional LinkedIn caption generator.
Given a brief achievement or story (Input), write an inspiring LinkedIn caption with:
- Title with emoji
- Motivational 2â€“3 line body
- 3 bullet points (check emoji âœ…)
- Concluding motivation
- ğŸš€ ending
- 10+ descriptive hashtags starting with hashtag#

Keep it personal, enthusiastic, and detailed.

"""

# ğŸ” Add 3 few-shot examples
for _, row in examples.iterrows():
    few_shot_prompt += f"""Input: {row['Content']}

Output:
ğŸ† {row['Type']} at {row['Location']}!
What a journey it's been â€“ {row['Name']} took one bold step forward.
âœ… {row['Title']}
âœ… Shared insights
âœ… Inspired community
The impact continues to grow. Stay tuned! ğŸš€
hashtag#LinkedIn hashtag#Motivation hashtag#Leadership hashtag#Career hashtag#Success
---
"""

# ğŸ”¤ New input
new_input = "joined in TCS"

# ğŸ¯ Final Prompt
final_prompt = few_shot_prompt + f"Input: {new_input}\n\nOutput:\n"

# ğŸ§  Tokenize and generate
inputs = tokenizer(final_prompt, return_tensors="pt", max_length=1024, truncation=True)
output_ids = model.generate(**inputs, max_new_tokens=250, temperature=0.7)
output = tokenizer.decode(output_ids[0], skip_special_tokens=True)

# ğŸ“¢ Show output
print("ğŸ”¹ Generated LinkedIn Caption:\n")
print(output)

pip install transformers accelerate pandas

'''import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ğŸ“‚ Load your dataset
df = pd.read_csv("/content/Total Linkein data(in).csv")
df = df.dropna(subset=['Name', 'Title', 'Location', 'Type', 'Content'])

# âœ‚ï¸ Take few examples
examples = df.sample(min(3, len(df)), random_state=42)

# ğŸ§  Prompt construction
few_shot_prompt = """You are a professional LinkedIn caption generator.

Generate a human-like paragraph-style caption for a LinkedIn post.

Requirements:
- Start with a personal and relatable hook (no emojis)
- Keep tone inspirational, not overly formal
- Avoid emoji overload (max 1-2 if any)
- Make it human, story-like, concise (4â€“6 lines)
- End with relevant hashtags (starting with 'hashtag#', around 10)

Examples:
"""

# ğŸ” Add real examples from dataset
for _, row in examples.iterrows():
    few_shot_prompt += f"""
Input: {row['Content']}

Output:
I recently achieved something meaningful in my journey: {row['Type']} at {row['Location']}.
It reminded me of how growth often comes from stepping outside our comfort zones.
Iâ€™m grateful for the lessons learned and the amazing people I connected with.
Let's keep pushing boundaries and sharing our progress.
hashtag#Career hashtag#Growth hashtag#Inspiration hashtag#LearningJourney
---
"""

# ğŸ”¢ New user input
new_input = "LeetCode 50 days streak"

# ğŸ“¦ Final prompt
final_prompt = few_shot_prompt + f"\nInput: {new_input}\n\nOutput:\n"

# ğŸ§  Load tokenizer and model from Hugging Face (free model)
model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# ğŸ›  Inference pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# âš™ï¸ Generate output
output = generator(final_prompt, max_new_tokens=300, do_sample=True, temperature=0.7)[0]['generated_text']

# ğŸ§¼ Clean output (print only what comes after "Output:")
print("\nğŸ”¹ Generated LinkedIn Caption:\n")
print(output.split("Output:")[-1].strip())'''

import requests

url = "https://openrouter.ai/api/v1/chat/completions"
headers = {
    "Authorization": "Your model or api",
    "Content-Type": "application/json",
    "HTTP-Referer": "https://your-site.com",  # Optional
    "X-Title": "My Test App"  # Optional
}
data = {
    "model": "meta-llama/llama-3-70b-instruct",
    "messages": [
        {"role": "user", "content": "What is the meaning of life?"}
    ]
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["choices"][0]["message"]["content"])

pip install openai

pip install transformers datasets scikit-learn torch

!pip install --upgrade transformers

pip install -U transformers

from torch.optim import AdamW

!pip install -U transformers

import transformers
print(transformers.__version__)

