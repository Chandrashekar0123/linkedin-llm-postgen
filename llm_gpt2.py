# -*- coding: utf-8 -*-
"""LLM-GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DOxAJBOXtMwHayJU0OXHMu9v4VghPKOU
"""

# 1️⃣ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ╔════════════════════════════════════════════════════════════════════════╗
# ║  LinkedIn‑Post‑LLM – Extract post text content from CSV to .txt file   ║
# ╚════════════════════════════════════════════════════════════════════════╝
import os, csv, random, subprocess, pathlib
import pandas as pd

# 2️⃣ Define base and data directories
BASE_DIR = pathlib.Path("/content/drive/MyDrive/linkedin_post_llm")
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

# 3️⃣ Download CSV if not already downloaded
CSV_URL = "https://raw.githubusercontent.com/mhlieu/LinkedIn-post-analysis/main/LINKEDIN_POSTS_all.csv"
CSV_PATH = DATA_DIR / "posts.csv"
if not CSV_PATH.exists():
    subprocess.run(["curl", "-L", "-o", str(CSV_PATH), CSV_URL], check=True)

# 4️⃣ Load CSV and print column names
df = pd.read_csv(CSV_PATH)
print("Available columns:", list(df.columns))

# 5️⃣ Set correct column name for post content
TEXT_COLUMN = "TEXT"  # ✅ Corrected from "post_content"

if TEXT_COLUMN not in df.columns:
    raise ValueError(f"Column '{TEXT_COLUMN}' not found in CSV. Please check the column names printed above.")

# 6️⃣ Save post contents to TXT file
TXT_PATH = DATA_DIR / "linkedin_posts.txt"
lines = df[TEXT_COLUMN].dropna().astype(str).apply(lambda x: x.strip().replace("\n", ""))
lines = lines[lines != ""]  # Remove empty strings
lines = lines.tolist()
random.shuffle(lines)

with open(TXT_PATH, "w", encoding="utf-8") as f:
    f.write("\n".join(lines))

# 7️⃣ Preview
print(f"\nSaved {len(lines)} posts → {TXT_PATH}")
print("First 3 lines:\n" + "-"*40)
for preview in lines[:3]:
    print(preview)

!pip install -U transformers datasets --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset
import pandas as pd
import os

os.environ["WANDB_DISABLED"] = "true"

# Load data
file_path = "/content/drive/MyDrive/linkedin_post_llm/data/linkedin_posts.txt"
with open(file_path, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

df = pd.DataFrame({"text": lines})
dataset = Dataset.from_pandas(df)

# Load GPT tokenizer and model
model_checkpoint = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have pad_token by default

def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

model = AutoModelForCausalLM.from_pretrained(model_checkpoint)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/linkedin_post_llm/finetuned_model",
    overwrite_output_dir=True,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# Save model + tokenizer
trainer.save_model("/content/drive/MyDrive/linkedin_post_llm/finetuned_model")
tokenizer.save_pretrained("/content/drive/MyDrive/linkedin_post_llm/finetuned_model")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Path to fine-tuned model
model_path = "/content/drive/MyDrive/linkedin_post_llm/finetuned_model"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

import torch

def generate_linkedin_post(prompt, max_length=100, temperature=0.8, top_p=0.95, top_k=50):
    # Encode the prompt
    input_ids = tokenizer.encode(prompt, return_tensors="pt")

    # Generate text
    output = model.generate(
        input_ids=input_ids,
        max_length=max_length,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        pad_token_id=tokenizer.eos_token_id,
        num_return_sequences=1
    )

    # Decode the output
    return tokenizer.decode(output[0], skip_special_tokens=True)

from transformers import Trainer

# Reuse existing trainer with eval_dataset
eval_results = trainer.evaluate(eval_dataset=tokenized_dataset)
print(f"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}")

prompts = [
    "Excited to share my new project on AI: ",
    "Just completed an amazing internship experience at ",
    "Here’s what I learned from my last side project: "
]

for prompt in prompts:
    generated_post = generate_linkedin_post(prompt)
    print("\nPrompt:", prompt)
    print("Generated:", generated_post)

prompt = "Thrilled to announce"
generated_text = generate_linkedin_post(prompt)
print("🔹 Generated LinkedIn Post:\n", generated_text)

for i in range(3):
    print(f"📝 Version {i+1}:\n{generate_linkedin_post('Excited to share')} \n")

"""# **Method 2**"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

prompt = "Career Growth Tip:"
inputs = tokenizer(prompt, return_tensors="pt")

# Create attention mask to avoid warning
attention_mask = torch.ones_like(inputs["input_ids"])

outputs = model.generate(
    inputs["input_ids"],
    attention_mask=attention_mask,
    max_length=inputs["input_ids"].shape[1] + 80,  # generate up to 80 tokens
    do_sample=True,
    top_p=0.9,
    temperature=0.85,
    top_k=50,
    no_repeat_ngram_size=3,
    pad_token_id=tokenizer.eos_token_id
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Simple way to add some hashtags (can be made smarter)
hashtags = " #CareerGrowth #Learning #ProfessionalDevelopment #LinkedInTips"

print("Generated LinkedIn Post:")
print(generated_text + hashtags)

# After training
model.save_pretrained("/content/drive/MyDrive/linkedin_post_llm/finetuned_model")
tokenizer.save_pretrained("/content/drive/MyDrive/linkedin_post_llm/finetuned_model")

!pip install streamlit pyngrok transformers

!cp -r /content/drive/MyDrive/linkedin_post_llm /content/

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch
# 
# # Load model and tokenizer only once
# @st.cache_resource
# def load_model():
#     model_path = "linkedin_post_llm"
#     tokenizer = AutoTokenizer.from_pretrained(model_path)
#     model = AutoModelForCausalLM.from_pretrained(model_path)
#     return tokenizer, model
# 
# tokenizer, model = load_model()
# 
# st.set_page_config(page_title="LinkedIn Post Generator", layout="centered")
# st.title("💼 LinkedIn Post Generator")
# 
# user_input = st.text_area("Enter keywords or prompt", height=150)
# 
# temperature = st.slider("Creativity (Temperature)", 0.5, 1.5, 1.0)
# max_length = st.slider("Max Length", 50, 300, 150)
# 
# if st.button("🚀 Generate Post"):
#     if not user_input.strip():
#         st.warning("Please enter a prompt.")
#     else:
#         input_ids = tokenizer.encode(user_input, return_tensors="pt")
#         with torch.no_grad():
#             output = model.generate(
#                 input_ids,
#                 max_length=max_length,
#                 temperature=temperature,
#                 do_sample=True,
#                 top_k=50,
#                 top_p=0.95,
#                 pad_token_id=tokenizer.eos_token_id
#             )
#         generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
#         st.subheader("📝 Generated LinkedIn Post")
#         st.success(generated_text)
# 
# st.markdown("---")
# st.markdown("Built with ❤️ using Hugging Face + Streamlit")

!ngrok config add-authtoken 2rwq2V6fDFhbDZDmrx5mxxwhBwo_4wPq1yP8imGyUA4Z1xPN4

from pyngrok import ngrok
import time
import os

# Kill any existing Streamlit instances
!pkill streamlit

# Run Streamlit app
!streamlit run app.py &

# Wait for the app to load
time.sleep(5)

# Open ngrok tunnel
public_url = ngrok.connect(8501)
print("🔗 App is live at:", public_url)

